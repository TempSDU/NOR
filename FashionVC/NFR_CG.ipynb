{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import tensorflow.contrib.keras as keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from trec_eval import trec_eval\n",
    "import nltk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "config = tf.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_samples(num_samples,toplist,downlist,combinationlist):\n",
    "    sampledata = []\n",
    "    num = 0\n",
    "    while num < num_samples:\n",
    "        top = random.sample(toplist,1)[0]\n",
    "        down = random.sample(downlist,1)[0]\n",
    "        if top+down not in combinationlist:\n",
    "            sampledata.append((top,down,-1))\n",
    "            num += 1\n",
    "    return sampledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_to_input(batch,imglist,topidlist,downidlist):\n",
    "    img1 = []#for top\n",
    "    img2 = []#for down\n",
    "    img1id = []\n",
    "    img2id = []\n",
    "    label = []\n",
    "    for instance in batch:\n",
    "        img1.append(imglist[instance[0]])\n",
    "        img2.append(imglist[instance[1]])\n",
    "        img1id.append(topidlist[instance[0]])\n",
    "        img2id.append(downidlist[instance[1]])\n",
    "        commentid = instance[2]\n",
    "        if commentid == -1:\n",
    "            label.append([1,0])\n",
    "        else:\n",
    "            label.append([0,1])\n",
    "    return np.array(img1),np.array(img2),np.array(img1id),np.array(img2id),np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(data,batch_size,toplist,downlist,combinationlist,imglist,topidlist,downidlist):\n",
    "    datacopy = copy.copy(data)\n",
    "    datacopy.extend(negative_samples(len(datacopy),toplist,downlist,combinationlist))\n",
    "    random.shuffle(datacopy)\n",
    "    for batch_i in range(0,len(datacopy)//batch_size+1):\n",
    "        start_i = batch_i*batch_size\n",
    "        batch = datacopy[start_i:start_i+batch_size]         \n",
    "        yield batch_to_input(batch,imglist,topidlist,downidlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_evaluation_batch(fixitem,itemlist,state,imglist,topidlist,downidlist):\n",
    "    img1 = []\n",
    "    img2 = []\n",
    "    img1id = []\n",
    "    img2id = []\n",
    "    if state == 0:#top,downs\n",
    "        for item in itemlist:\n",
    "            img1.append(imglist[fixitem])\n",
    "            img2.append(imglist[item])\n",
    "            img1id.append(topidlist[fixitem])\n",
    "            img2id.append(downidlist[item])\n",
    "    if state == 1:#down,tops\n",
    "        for item in itemlist:\n",
    "            img1.append(imglist[item])\n",
    "            img2.append(imglist[fixitem])\n",
    "            img1id.append(topidlist[item])\n",
    "            img2id.append(downidlist[fixitem])\n",
    "    return np.array(img1),np.array(img2),np.array(img1id),np.array(img2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(label,prediction):\n",
    "    return (label.argmax(axis=1) == prediction.argmax(axis=1)).sum()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_evaluation(data_path):  \n",
    "    with open(data_path,'r') as f:\n",
    "        content = f.readlines()\n",
    "    data = {}\n",
    "    orderlist = []\n",
    "    labellist = {}\n",
    "    query_number = 0\n",
    "    for line in content:\n",
    "        line = line[:-2].split('\\t')\n",
    "        if data.get(line[0]) != None:\n",
    "            data[line[0]].append(line[1])\n",
    "        else:\n",
    "            data[line[0]] = [line[1]] \n",
    "            labellist[query_number] = {}\n",
    "            query_number += 1\n",
    "            orderlist.append(line[0])\n",
    "        if int(line[2]) == 1:\n",
    "            labellist[query_number-1][line[1]] = 1\n",
    "        else:\n",
    "            labellist[query_number-1][line[1]] = 0\n",
    "    return data,orderlist,labellist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trec_evaluation(qrel_file_path,trec_file_path,trec):\n",
    "    with open(trec_file_path,'w') as f:\n",
    "        i = 0\n",
    "        while i < len(trec):\n",
    "            j = 0 \n",
    "            while j < len(trec[i]):\n",
    "                f.write(str(i)+' '+'Q0 '+trec[i][j][0]+' '+str(j+1)+' '+str(trec[i][j][1])+' '+'Exp'+'\\n')#注意排序从1开始\n",
    "                j += 1\n",
    "            i += 1   \n",
    "    result = trec_eval(qrel_file_path,trec_file_path)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_evaluation(labellist,trec):\n",
    "    query_number = 0\n",
    "    record = []\n",
    "    while query_number < len(trec):\n",
    "        negative = 0\n",
    "        temp = []\n",
    "        for combination in trec[query_number]:\n",
    "            if labellist[query_number][combination[0]] == 1:\n",
    "                temp.append(negative)\n",
    "            else:\n",
    "                negative += 1\n",
    "        record.extend([(negative-val)/float(negative) for val in temp])\n",
    "        query_number += 1\n",
    "    auc = np.array(record).mean()\n",
    "    print(auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toplist = []\n",
    "topidlist = {}\n",
    "with open('dataset/toplist.dat','r') as f:#in toplist, the first col is img_name of top, the second col is comments_index\n",
    "    content = f.readlines()\n",
    "for line in content:\n",
    "    line = line[:-2]\n",
    "    toplist.append(line)\n",
    "    topidlist[line] = len(topidlist)\n",
    "toplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topidlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downlist = []\n",
    "downidlist = {}\n",
    "with open('dataset/downlist.dat','r') as f:#in downlist, the first col is img_name of down(i.e. bottom), the second col is comments_index\n",
    "    content = f.readlines()\n",
    "for line in content:\n",
    "    line = line[:-2]\n",
    "    downlist.append(line)\n",
    "    downidlist[line] = len(downidlist)\n",
    "downlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downidlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combinationlist = set()\n",
    "with open('dataset/combinationlist.dat','r') as f:#in combinationlist, the first col is img_name of top, the second col is img_name of down(i.e. bottom), the third col is comments_index    \n",
    "    content = f.readlines()\n",
    "for line in content:\n",
    "    line = line[:-2].split('\\t')\n",
    "    combinationlist.add(line[0]+line[1])\n",
    "combinationlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imglist = {}\n",
    "for img_idx in toplist:\n",
    "    img = Image.open('img/'+img_idx+'.jpg')\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = np.array(img)\n",
    "    img = img/255.0\n",
    "    imglist[img_idx] = img\n",
    "for img_idx in downlist:\n",
    "    img = Image.open('img/'+img_idx+'.jpg')\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = np.array(img)\n",
    "    img = img/255.0\n",
    "    imglist[img_idx] = img\n",
    "imglist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input():\n",
    "    img1 = tf.placeholder(tf.float32,[None,150,150,3],'img1')\n",
    "    img2 = tf.placeholder(tf.float32,[None,150,150,3],'img2')\n",
    "    img1id = tf.placeholder(tf.int32,[None,],'img1id')\n",
    "    img2id = tf.placeholder(tf.int32,[None,],'img2id')\n",
    "    label = tf.placeholder(tf.float32,[None,2],'label')\n",
    "    learning_rate = tf.placeholder(tf.float32,[],name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32,[],name='keep_prob')\n",
    "    return img1,img2,img1id,img2id,label,learning_rate,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractor(img):\n",
    "    conv1 = keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu',data_format='channels_last',kernel_initializer='glorot_normal')(img)\n",
    "    conv2 = keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu',data_format='channels_last',kernel_initializer='glorot_normal')(conv1)\n",
    "    pool1 = keras.layers.MaxPool2D(pool_size=(16,16),padding='same')(conv1)\n",
    "    pool2 = keras.layers.MaxPool2D(pool_size=(16,16),padding='same')(conv2)\n",
    "    concat = keras.layers.Concatenate(axis=-1)([pool1,pool2])\n",
    "    globalpool = keras.layers.GlobalAveragePooling2D()(concat)\n",
    "    return concat,globalpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_to_image_attention(conv,globalpool):#conv=[batch_size,14,14,64]，globalpool=[batch_size,64] \n",
    "    weights1 = tf.get_variable('weights1',shape=[64,64],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    weights2 = tf.get_variable('weights2',shape=[64,64],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    weights3 = tf.get_variable('weights3',shape=[64,1],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    attn_from = tf.matmul(globalpool,weights1)#attn_form=[batch_size,64]\n",
    "    features = keras.layers.Reshape([-1,64])(conv)#features=[batch_size,196,64] \n",
    "    attn_to = tf.matmul(tf.reshape(features,[-1,64]),weights2)#tf.reshape(features,[-1,64])=[batch_size*196,64]，attn_to=[batch_size*196,64]\n",
    "    attn_from  = tf.expand_dims(attn_from,1)#attn_from=[batch_size,1,64]\n",
    "    attn_to = tf.reshape(attn_to,tf.shape(features))#attn_to=[batch_size,196,64] \n",
    "    attn_logit = tf.add(attn_from,attn_to)#attn_logit=[batch_size,196,64]\n",
    "    attn_logit = tf.reshape(attn_logit,[-1,64])#attn_logit=[batch_size*196,64]\n",
    "    attn_logit = tf.tanh(attn_logit)\n",
    "    attn_weight = tf.matmul(attn_logit,weights3)#attn_weight=[batch_size*196,1]\n",
    "    attn_weight = tf.reshape(attn_weight,shape=[tf.shape(conv)[0],tf.shape(conv)[1]*tf.shape(conv)[2]])#attn_weight=[batch_size,196]\n",
    "    attn_weight = tf.nn.softmax(attn_weight,name='attention_img2img')\n",
    "    attn_weight = tf.expand_dims(attn_weight,-1)#attn_weight=[batch_size,196,1]\n",
    "    attn_conv = tf.multiply(features,attn_weight)#attn_conv=[batch_size,196,64]\n",
    "    attn_conv = tf.reduce_sum(attn_conv,axis=1)#attn_conv=[batch_size,64]\n",
    "    return features,attn_conv#e=v^Ttanh(W1s+W2h)，a=softmax(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img2vec(conv):\n",
    "    extractor_output = keras.layers.Dense(300,activation='relu',kernel_initializer='glorot_normal')(conv)\n",
    "    return extractor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_embedding(img1id,img2id):\n",
    "    top_embedding_matrix = tf.get_variable('top_embedding_matrix',shape=[len(toplist),embedding_size],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    down_embedding_matrix = tf.get_variable('down_embedding_matrix',shape=[len(downlist),embedding_size],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    img1_embedding = tf.nn.embedding_lookup(top_embedding_matrix,img1id)\n",
    "    img2_embedding = tf.nn.embedding_lookup(down_embedding_matrix,img2id)\n",
    "    return img1_embedding,img2_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(extractor_output,keep_prob):\n",
    "    dense = keras.layers.Dense(256,activation='relu',kernel_initializer='glorot_normal')(extractor_output)\n",
    "    dropout = tf.nn.dropout(dense,keep_prob)\n",
    "    classifier_output = keras.layers.Dense(2,activation='softmax',kernel_initializer='glorot_normal')(dropout)\n",
    "    return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(classifier_output,label):\n",
    "    classifier_loss = tf.reduce_mean(tf.contrib.keras.losses.categorical_crossentropy(label,classifier_output),name='classifier_loss')\n",
    "    tv = tf.trainable_variables()\n",
    "    reg_loss = tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "    loss = tf.add(classifier_loss,0.00001*reg_loss,name='loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(loss,learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    capped_gradients = [(tf.clip_by_value(grad,-5.,5.),var) for grad,var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(classifier_output):\n",
    "    prediction = tf.identity(classifier_output,name='prediction')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    tf.set_random_seed(1)\n",
    "    with tf.name_scope('inputs'):\n",
    "        img1,img2,img1id,img2id,label,learning_rate,keep_prob = get_input()\n",
    "    with tf.name_scope('extractor'):\n",
    "        with tf.variable_scope('extractor'):\n",
    "            conv_img1,globalpool_img1 = extractor(img1)\n",
    "        with tf.variable_scope('extractor',reuse=True):\n",
    "            conv_img2,globalpool_img2 = extractor(img2)\n",
    "        with tf.variable_scope('image_to_image_attention'):\n",
    "            features_img1,attn_conv_img1 = image_to_image_attention(conv_img1,globalpool_img2)\n",
    "        with tf.variable_scope('image_to_image_attention',reuse=True):\n",
    "            features_img2,attn_conv_img2 = image_to_image_attention(conv_img2,globalpool_img1)\n",
    "        with tf.variable_scope('img2vec'):\n",
    "            extractor_output_img1 = img2vec(attn_conv_img1)\n",
    "        with tf.variable_scope('img2vec',reuse=True):\n",
    "            extractor_output_img2 = img2vec(attn_conv_img2)\n",
    "        with tf.variable_scope('img_embedding'):\n",
    "            img1_embedding,img2_embedding = img_embedding(img1id,img2id)\n",
    "        encoder_output = tf.concat([features_img1,features_img2],axis=1,name='encoder_output')\n",
    "        extractor_output = tf.concat([extractor_output_img1,extractor_output_img2,img1_embedding,img2_embedding],axis=1,name='extractor_output')\n",
    "    with tf.name_scope('classifier'):\n",
    "        classifier_output = classifier(extractor_output,keep_prob)\n",
    "    with tf.name_scope('prediction'):\n",
    "        prediction = prediction(classifier_output)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = loss(classifier_output,label)\n",
    "    with tf.name_scope('optimizer'): \n",
    "        train_op = optimizer(loss,learning_rate)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('dataset/traindata.dat','r') as f:#in traindata, the first col is img_name of top, the second col is img_name of down(i.e. bottom)  \n",
    "    content = f.readlines()\n",
    "traindata = []\n",
    "for line in content:\n",
    "    line = line[:-2].split('\\t')\n",
    "    traindata.append((line[0],line[1],1))\n",
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tops_qrel_file_path = 'evaluation/devdata_tops_qrel.dat'\n",
    "tops_trec_file_path = 'evaluation/devdata_tops_trec.dat'\n",
    "#downs_qrel_file_path = 'evaluation/devdata_downs_qrel.dat'\n",
    "#downs_trec_file_path = 'evaluation/devdata_downs_trec.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'dataset/devdata_tops.dat'\n",
    "dev_tops_data,tops_orderlist,tops_labellist = prepare_evaluation(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_path = 'dataset/devdata_downs.dat'\n",
    "#dev_downs_data,downs_orderlist,downs_labellist = prepare_evaluation(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 30\n",
    "rate = 1.0\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost_list = []\n",
    "auc_tops = []\n",
    "trec_evals_tops = []\n",
    "#trec_evals_downs = []\n",
    "#auc_downs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 'checkpoint/trained_model.ckpt'\n",
    "with tf.Session(graph=train_graph,config=config) as sess:\n",
    "    writer = tf.summary.FileWriter('checkpoint/',sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=30)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(time.localtime())\n",
    "    classifier_loss = train_graph.get_tensor_by_name('loss/classifier_loss:0')\n",
    "    for epoch in range(epochs):\n",
    "        train_cost = 0\n",
    "        temp_cost_list = []\n",
    "        step = 0\n",
    "        for _,(x_i1,x_i2,x_id1,x_id2,y_l) in enumerate(get_batches(traindata,batch_size,toplist,downlist,combinationlist,imglist,topidlist,downidlist)):\n",
    "            _,cost = sess.run([train_op,classifier_loss],{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,label:y_l,learning_rate:lr,keep_prob:rate})\n",
    "            train_cost += cost\n",
    "            step += 1\n",
    "            if step%1000 == 0:\n",
    "                temp_cost_list.append(train_cost/step)\n",
    "                print(str(train_cost/step)+' '+'pass!')\n",
    "        temp_cost_list.append(train_cost/step)\n",
    "        cost_list.append(temp_cost_list)\n",
    "        print('Epoch {}/{} - Training Loss: {:.3f}'.format(epoch+1,epochs,train_cost/step))\n",
    "        saver.save(sess,checkpoint,global_step=epoch+1)\n",
    "        print('Model Trained and Saved')\n",
    "        print(time.localtime())\n",
    "        #validation      \n",
    "        tops_trec = {}\n",
    "        query_number = 0\n",
    "        step = 0\n",
    "        for top in tops_orderlist:\n",
    "            downsoftop = dev_tops_data[top]\n",
    "            probabilitylist = {}\n",
    "            for batch_i in range(len(downsoftop)//batch_size+1):\n",
    "                start_i = batch_i*batch_size\n",
    "                downs = downsoftop[start_i:start_i+batch_size]\n",
    "                x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(top,downs,0,imglist,topidlist,downidlist)\n",
    "                seq_len = [30]*len(x_i1)\n",
    "                prob = sess.run(prediction,{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,keep_prob:1.0})\n",
    "                j = 0\n",
    "                for down in downs:\n",
    "                    probabilitylist[down] = prob[j][1]\n",
    "                    j += 1 \n",
    "                step += 1\n",
    "                if step%1000 == 0:\n",
    "                    print('pass!')\n",
    "            tops_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "            del probabilitylist,downsoftop\n",
    "            query_number += 1\n",
    "        auc_tops.append(auc_evaluation(tops_labellist,tops_trec))\n",
    "        trec_evals_tops.append(trec_evaluation(tops_qrel_file_path,tops_trec_file_path,tops_trec))\n",
    "        del tops_trec\n",
    "        '''\n",
    "        downs_trec = {}\n",
    "        query_number = 0\n",
    "        step = 0\n",
    "        for down in downs_orderlist:\n",
    "            topsofdown = dev_downs_data[down]\n",
    "            probabilitylist = {}\n",
    "            for batch_i in range(len(topsofdown)//batch_size+1):\n",
    "                start_i = batch_i*batch_size\n",
    "                tops = topsofdown[start_i:start_i+batch_size]\n",
    "                x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(down,tops,1,imglist,topidlist,downidlist)\n",
    "                prob = sess.run(prediction,{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,keep_prob:1.0})\n",
    "                j = 0\n",
    "                for top in tops:\n",
    "                    probabilitylist[top] = prob[j][1]\n",
    "                    j += 1\n",
    "                step += 1\n",
    "                if step%1000 == 0:\n",
    "                    print('pass!')\n",
    "            downs_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "            del probabilitylist,topsofdown\n",
    "            query_number += 1\n",
    "        auc_downs.append(auc_evaluation(downs_labellist,downs_trec))\n",
    "        trec_evals_downs.append(trec_evaluation(downs_qrel_file_path,downs_trec_file_path,downs_trec))\n",
    "        del downs_trec\n",
    "        '''\n",
    "        #validation      \n",
    "        print(time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tops_qrel_file_path = 'evaluation/testdata_tops_qrel.dat'\n",
    "tops_trec_file_path = 'evaluation/testdata_tops_trec.dat'\n",
    "downs_qrel_file_path = 'evaluation/testdata_downs_qrel.dat'\n",
    "downs_trec_file_path = 'evaluation/testdata_downs_trec.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'dataset/testdata_tops.dat'#in testdata_tops, the first col is img_name of top, the second col is img_name of down(i.e. bottom), the third col is rel(1 relevant, 0 irrelevant)  \n",
    "test_tops_data,tops_orderlist,tops_labellist = prepare_evaluation(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'dataset/testdata_downs.dat'#in testdata_downs, the first col is img_name of down(i.e. bottom), the second col is img_name of top, the third col is rel(1 relevant, 0 irrelevant) \n",
    "test_downs_data,downs_orderlist,downs_labellist = prepare_evaluation(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(time.localtime())\n",
    "checkpoint = 'checkpoint/trained_model.ckpt-23'\n",
    "test_graph = tf.Graph()\n",
    "with tf.Session(graph=test_graph,config=config) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "    loader.restore(sess,checkpoint)\n",
    "    img1 = test_graph.get_tensor_by_name('inputs/img1:0')\n",
    "    img2 = test_graph.get_tensor_by_name('inputs/img2:0')\n",
    "    img1id = test_graph.get_tensor_by_name('inputs/img1id:0')\n",
    "    img2id = test_graph.get_tensor_by_name('inputs/img2id:0')\n",
    "    keep_prob = test_graph.get_tensor_by_name('inputs/keep_prob:0')\n",
    "    prediction = test_graph.get_tensor_by_name('prediction/prediction:0')\n",
    "    batch_size = 64\n",
    "    tops_trec = {}\n",
    "    query_number = 0\n",
    "    step = 0\n",
    "    for top in tops_orderlist:\n",
    "        downsoftop = test_tops_data[top]\n",
    "        probabilitylist = {}\n",
    "        for batch_i in range(len(downsoftop)//batch_size+1):\n",
    "            start_i = batch_i*batch_size\n",
    "            downs = downsoftop[start_i:start_i+batch_size]\n",
    "            x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(top,downs,0,imglist,topidlist,downidlist)\n",
    "            prob = sess.run(prediction,{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,keep_prob:1.0})\n",
    "            j = 0\n",
    "            for down in downs:\n",
    "                probabilitylist[down] = prob[j][1]\n",
    "                j += 1 \n",
    "            step += 1\n",
    "            if step%1000 == 0:\n",
    "                print('pass!')\n",
    "        tops_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "        del probabilitylist,downsoftop\n",
    "        query_number += 1\n",
    "    auc_evaluation(tops_labellist,tops_trec)\n",
    "    trec_evaluation(tops_qrel_file_path,tops_trec_file_path,tops_trec)\n",
    "    del tops_trec\n",
    "    downs_trec = {}\n",
    "    query_number = 0\n",
    "    step = 0\n",
    "    for down in downs_orderlist:\n",
    "        topsofdown = test_downs_data[down]\n",
    "        probabilitylist = {}\n",
    "        for batch_i in range(len(topsofdown)//batch_size+1):\n",
    "            start_i = batch_i*batch_size\n",
    "            tops = topsofdown[start_i:start_i+batch_size]\n",
    "            x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(down,tops,1,imglist,topidlist,downidlist)\n",
    "            prob = sess.run(prediction,{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,keep_prob:1.0})\n",
    "            j = 0\n",
    "            for top in tops:\n",
    "                probabilitylist[top] = prob[j][1]\n",
    "                j += 1\n",
    "            step += 1\n",
    "            if step%1000 == 0:\n",
    "                print('pass!')\n",
    "        downs_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "        del probabilitylist,topsofdown\n",
    "        query_number += 1\n",
    "    auc_evaluation(downs_labellist,downs_trec)\n",
    "    trec_evaluation(downs_qrel_file_path,downs_trec_file_path,downs_trec)\n",
    "    del downs_trec\n",
    "print(time.localtime())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
