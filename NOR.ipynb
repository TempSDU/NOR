{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import tensorflow.contrib.keras as keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "from trec_eval import trec_eval\n",
    "import nltk\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_comments(file_name):\n",
    "    with open(file_name,'r') as f:\n",
    "        file_content = f.readlines()\n",
    "    comments = []\n",
    "    for line in file_content:\n",
    "        comments.append(line[:-1].split())\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(file_name,min_num):\n",
    "    with open(file_name,'r') as f:\n",
    "        file_content = f.readlines()\n",
    "    word_to_int = {}\n",
    "    int_to_word = {}\n",
    "    words_num = 0\n",
    "    for line in file_content:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if int(line[2]) >= min_num:\n",
    "            word_to_int[line[1]] = int(line[0])\n",
    "            int_to_word[int(line[0])] = line[1] \n",
    "            words_num += 1\n",
    "        else:\n",
    "            break\n",
    "    word_to_int['<PAD>'] = 0\n",
    "    word_to_int['<UNK>'] = words_num+1\n",
    "    word_to_int['<GO>'] = words_num+2\n",
    "    word_to_int['<EOS>'] = words_num+3\n",
    "    int_to_word[0] = '<PAD>'\n",
    "    int_to_word[words_num+1] = '<UNK>'\n",
    "    int_to_word[words_num+2] = '<GO>'\n",
    "    int_to_word[words_num+3] = '<EOS>'\n",
    "    return word_to_int,int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_comments(comments,word_to_int,int_to_word):\n",
    "    comments_to_int = []\n",
    "    for comment in comments:\n",
    "        comment_to_int = [word_to_int[word] if word_to_int.get(word) != None else word_to_int['<UNK>'] for word in comment]  \n",
    "        comment_to_int.insert(0,word_to_int['<GO>'])\n",
    "        comment_to_int.append(word_to_int['<EOS>'])\n",
    "        comments_to_int.append(comment_to_int)\n",
    "    return comments_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_samples(num_samples,toplist,downlist,combinationlist):\n",
    "    sampledata = []\n",
    "    num = 0\n",
    "    while num < num_samples:\n",
    "        top = random.sample(toplist,1)[0]\n",
    "        down = random.sample(downlist,1)[0]\n",
    "        if top+down not in combinationlist:\n",
    "            sampledata.append((top,down,-1))\n",
    "            num += 1\n",
    "    return sampledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch,pad_int):\n",
    "    max_length = max([len(comment) for comment in batch])\n",
    "    pad_batch = pad_sequences(batch,maxlen=max_length,value=pad_int,padding='post')\n",
    "    return pad_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_to_input(batch,comments,imglist,topidlist,downidlist,pad_int):\n",
    "    img1 = []#for top\n",
    "    img2 = []#for down\n",
    "    img1id = []\n",
    "    img2id = []\n",
    "    label = []\n",
    "    sequence = []\n",
    "    sequence_length = []\n",
    "    weight = []\n",
    "    for instance in batch:\n",
    "        img1.append(imglist[instance[0]])\n",
    "        img2.append(imglist[instance[1]])\n",
    "        img1id.append(topidlist[instance[0]])\n",
    "        img2id.append(downidlist[instance[1]])\n",
    "        commentid = instance[2]\n",
    "        if commentid == -1:\n",
    "            label.append([1,0])\n",
    "            weight.append(0)\n",
    "        else:\n",
    "            label.append([0,1])\n",
    "            weight.append(1)\n",
    "        sequence.append(comments[commentid])\n",
    "        sequence_length.append(len(comments[commentid])-1)\n",
    "    sequence = pad_batch(sequence,pad_int)\n",
    "    sequence_input = sequence[:,:-1]\n",
    "    sequence_output = sequence[:,1:]\n",
    "    max_sequence_length = np.max(sequence_length)\n",
    "    return np.array(img1),np.array(img2),np.array(img1id),np.array(img2id),np.array(label),sequence_input,sequence_output,sequence_length,max_sequence_length,np.array(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(data,batch_size,comments,toplist,downlist,combinationlist,imglist,topidlist,downidlist,pad_int):\n",
    "    datacopy = copy.copy(data)\n",
    "    datacopy.extend(negative_samples(len(datacopy),toplist,downlist,combinationlist))\n",
    "    random.shuffle(datacopy)\n",
    "    for batch_i in range(0,len(datacopy)//batch_size+1):\n",
    "        start_i = batch_i*batch_size\n",
    "        batch = datacopy[start_i:start_i+batch_size]          \n",
    "        yield batch_to_input(batch,comments,imglist,topidlist,downidlist,pad_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_evaluation_batch(fixitem,itemlist,state,imglist,topidlist,downidlist):\n",
    "    img1 = []\n",
    "    img2 = []\n",
    "    img1id = []\n",
    "    img2id = []\n",
    "    if state == 0:#top,downs\n",
    "        for item in itemlist:\n",
    "            img1.append(imglist[fixitem])\n",
    "            img2.append(imglist[item])\n",
    "            img1id.append(topidlist[fixitem])\n",
    "            img2id.append(downidlist[item])\n",
    "    if state == 1:#down,tops\n",
    "        for item in itemlist:\n",
    "            img1.append(imglist[item])\n",
    "            img2.append(imglist[fixitem])\n",
    "            img1id.append(topidlist[item])\n",
    "            img2id.append(downidlist[fixitem])\n",
    "    return np.array(img1),np.array(img2),np.array(img1id),np.array(img2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_seq_to_word_seq(id_seq,id_vocab,eos):\n",
    "    index = 0\n",
    "    while index < len(id_seq):\n",
    "        if id_seq[index] == eos:\n",
    "            break\n",
    "        index += 1\n",
    "    valid_id_seq = id_seq[:index+1]\n",
    "    return ' '.join([id_vocab[id] for id in valid_id_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(label,prediction):\n",
    "    return (label.argmax(axis=1) == prediction.argmax(axis=1)).sum()/float(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_evaluation(data_path,comments,int_to_word,word_to_int):\n",
    "    with open(data_path,'r') as f:\n",
    "        content = f.readlines()\n",
    "    data = {}\n",
    "    orderlist = []\n",
    "    model_comments = {}\n",
    "    labellist = {}\n",
    "    query_number = 0\n",
    "    for line in content:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if data.get(line[0]) != None:\n",
    "            data[line[0]].append(line[1])\n",
    "        else:\n",
    "            data[line[0]] = [line[1]] \n",
    "            labellist[query_number] = {}\n",
    "            query_number += 1\n",
    "            orderlist.append(line[0])\n",
    "        if int(line[2]) == 1:\n",
    "            model_comments[(line[0],line[1])] = [id_seq_to_word_seq(comments[int(comment)],int_to_word,word_to_int['<EOS>']).split()[1:-1] for comment in line[3].split('|')]\n",
    "            labellist[query_number-1][line[1]] = 1\n",
    "        else:\n",
    "            labellist[query_number-1][line[1]] = 0\n",
    "    return data,orderlist,model_comments,labellist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trec_evaluation(qrel_file_path,trec_file_path,trec):\n",
    "    with open(trec_file_path,'w') as f:\n",
    "        i = 0\n",
    "        while i < len(trec):\n",
    "            j = 0 \n",
    "            while j < len(trec[i]):\n",
    "                f.write(str(i)+' '+'Q0 '+trec[i][j][0]+' '+str(j+1)+' '+str(trec[i][j][1])+' '+'Exp'+'\\n')\n",
    "                j += 1\n",
    "            i += 1   \n",
    "    result = trec_eval(qrel_file_path,trec_file_path)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bleu_evalaution(model_comments,system_comments,beamsearch):\n",
    "    select = {}\n",
    "    bleus = []\n",
    "    if beamsearch:\n",
    "        for combination,comments in system_comments.items():\n",
    "            scores = []\n",
    "            for comment in comments:\n",
    "                scores.append(nltk.translate.bleu_score.sentence_bleu(model_comments[combination],comment,weights=[1.0]))\n",
    "            scores = np.array(scores)\n",
    "            bleus.append(scores.max())\n",
    "            select[combination] = scores.argmax()#we only select the best for evaluation\n",
    "    else:\n",
    "        for combination,comment in system_comments.items():\n",
    "            bleus.append(nltk.translate.bleu_score.sentence_bleu(model_comments[combination],comment,weights=[1.0]))  \n",
    "    bleus = np.array(bleus)\n",
    "    print(bleus.mean())\n",
    "    return bleus.mean(),select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc_evaluation(labellist,trec):\n",
    "    query_number = 0\n",
    "    record = []\n",
    "    while query_number < len(trec):\n",
    "        negative = 0\n",
    "        temp = []\n",
    "        for combination in trec[query_number]:\n",
    "            if labellist[query_number][combination[0]] == 1:\n",
    "                temp.append(negative)\n",
    "            else:\n",
    "                negative += 1\n",
    "        record.extend([(negative-val)/float(negative) for val in temp])\n",
    "        query_number += 1\n",
    "    auc = np.array(record).mean()\n",
    "    print(auc)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_path = 'dataset/text.dat'\n",
    "vocab_path = 'dataset/vocab.dat'\n",
    "min_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments = read_comments(comments_path)\n",
    "comments.append([])\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_to_int,int_to_word = build_vocab(vocab_path,min_num)\n",
    "vocab_size = len(word_to_int)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = convert_comments(comments,word_to_int,int_to_word)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toplist = []\n",
    "topidlist = {}\n",
    "with open('dataset/toplist.dat','r') as f:#in toplist, the first col is img_name of top, the second col is comments_index\n",
    "    content = f.readlines()\n",
    "for line in content:\n",
    "    line = line.split('\\t')\n",
    "    toplist.append(line[0])\n",
    "    topidlist[line[0]] = len(topidlist)\n",
    "toplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topidlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downlist = []\n",
    "downidlist = {}\n",
    "with open('dataset/downlist.dat','r') as f:#in downlist, the first col is img_name of down(i.e. bottom), the second col is comments_index\n",
    "    content = f.readlines()\n",
    "for line in content:\n",
    "    line = line.split('\\t')\n",
    "    downlist.append(line[0])\n",
    "    downidlist[line[0]] = len(downidlist)\n",
    "downlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downidlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combinationlist = set()\n",
    "with open('dataset/combinationlist.dat','r') as f:#in combinationlist, the first col is img_name of top, the second col is img_name of down(i.e. bottom), the third col is comments_index    \n",
    "    content = f.readlines()\n",
    "for line in content:\n",
    "    line = line[:-1].split('\\t')\n",
    "    combinationlist.add(line[0]+line[1])\n",
    "combinationlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imglist = {}\n",
    "for img_idx in toplist:\n",
    "    img = Image.open('img/'+img_idx+'.jpg')\n",
    "    img = np.array(img)\n",
    "    img = img/255.0\n",
    "    imglist[img_idx] = img\n",
    "for img_idx in downlist:\n",
    "    img = Image.open('img/'+img_idx+'.jpg')\n",
    "    img = np.array(img)\n",
    "    img = img/255.0\n",
    "    imglist[img_idx] = img\n",
    "imglist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input():\n",
    "    img1 = tf.placeholder(tf.float32,[None,224,224,3],'img1')\n",
    "    img2 = tf.placeholder(tf.float32,[None,224,224,3],'img2')\n",
    "    img1id = tf.placeholder(tf.int32,[None,],'img1id')\n",
    "    img2id = tf.placeholder(tf.int32,[None,],'img2id')\n",
    "    label = tf.placeholder(tf.float32,[None,2],'label')\n",
    "    sequence_input = tf.placeholder(tf.int32,[None,None],name='sequence_input')\n",
    "    sequence_output = tf.placeholder(tf.int32,[None,None],name='sequence_output')\n",
    "    sequence_length = tf.placeholder(tf.int32,[None,],name='sequence_length')\n",
    "    max_sequence_length = tf.placeholder(tf.int32,[],name='max_sequence_length')\n",
    "    batch_size = tf.placeholder(tf.int32,[],name='batch_size')\n",
    "    learning_rate = tf.placeholder(tf.float32,[],name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32,[],name='keep_prob')\n",
    "    ratio_c = tf.placeholder(tf.float32,[],name='ratio_c')\n",
    "    ratio_g = tf.placeholder(tf.float32,[],name='ratio_g')\n",
    "    weight = tf.placeholder(tf.float32,[None,],name='weight')\n",
    "    flag = tf.placeholder(tf.bool,name='flag')\n",
    "    return img1,img2,img1id,img2id,label,sequence_input,sequence_output,sequence_length,max_sequence_length,batch_size,learning_rate,keep_prob,ratio_c,ratio_g,weight,flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractor(img):\n",
    "    conv1 = keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu',data_format='channels_last',kernel_initializer='glorot_normal')(img)\n",
    "    conv2 = keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='same',activation='relu',data_format='channels_last',kernel_initializer='glorot_normal')(conv1)\n",
    "    pool1 = keras.layers.MaxPool2D(pool_size=(16,16),padding='same')(conv1)\n",
    "    #print(pool1)\n",
    "    pool2 = keras.layers.MaxPool2D(pool_size=(16,16),padding='same')(conv2)\n",
    "    #print(pool2)\n",
    "    concat = keras.layers.Concatenate(axis=-1)([pool1,pool2])\n",
    "    #print(concat)\n",
    "    globalpool = keras.layers.GlobalAveragePooling2D()(concat)\n",
    "    #print(globalpool)\n",
    "    return concat,globalpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_to_image_attention(conv,globalpool):#conv=[batch_size,14,14,64]，globalpool=[batch_size,64]  \n",
    "    weights1 = tf.get_variable('weights1',shape=[64,64],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    weights2 = tf.get_variable('weights2',shape=[64,64],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    weights3 = tf.get_variable('weights3',shape=[64,1],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    attn_from = tf.matmul(globalpool,weights1)#attn_form=[batch_size,64]\n",
    "    features = keras.layers.Reshape([-1,64])(conv)#features=[batch_size,196,64] \n",
    "    attn_to = tf.matmul(tf.reshape(features,[-1,64]),weights2)#tf.reshape(features,[-1,64])=[batch_size*196,64]，attn_to=[batch_size*196,64]\n",
    "    attn_from  = tf.expand_dims(attn_from,1)#attn_from=[batch_size,1,64]\n",
    "    attn_to = tf.reshape(attn_to,tf.shape(features))#attn_to=[batch_size,196,64] \n",
    "    attn_logit = tf.add(attn_from,attn_to)#attn_logit=[batch_size,196,64]\n",
    "    attn_logit = tf.reshape(attn_logit,[-1,64])#attn_logit=[batch_size*196,64]\n",
    "    attn_logit = tf.tanh(attn_logit)\n",
    "    attn_weight = tf.matmul(attn_logit,weights3)#attn_weight=[batch_size*196,1]\n",
    "    attn_weight = tf.reshape(attn_weight,shape=[tf.shape(conv)[0],tf.shape(conv)[1]*tf.shape(conv)[2]])#attn_weight=[batch_size,196]\n",
    "    attn_weight = tf.nn.softmax(attn_weight,name='attention_img2img')   \n",
    "    attn_weight = tf.expand_dims(attn_weight,-1)#attn_weight=[batch_size,196,1]\n",
    "    attn_conv = tf.multiply(features,attn_weight)#attn_conv=[batch_size,196,64]\n",
    "    attn_conv = tf.reduce_sum(attn_conv,axis=1)#attn_conv=[batch_size,64]\n",
    "    return features,attn_conv#e=v^Ttanh(W1s+W2h)，a=softmax(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img2vec(conv):\n",
    "    extractor_output = keras.layers.Dense(300,activation='relu',kernel_initializer='glorot_normal')(conv)\n",
    "    return extractor_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_embedding(img1id,img2id):\n",
    "    top_embedding_matrix = tf.get_variable('top_embedding_matrix',shape=[len(toplist),embedding_size],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    down_embedding_matrix = tf.get_variable('down_embedding_matrix',shape=[len(downlist),embedding_size],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    img1_embedding = tf.nn.embedding_lookup(top_embedding_matrix,img1id)\n",
    "    img2_embedding = tf.nn.embedding_lookup(down_embedding_matrix,img2id)\n",
    "    return img1_embedding,img2_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(extractor_output,keep_prob):\n",
    "    dense = keras.layers.Dense(256,activation='relu',kernel_initializer='glorot_normal')(extractor_output)\n",
    "    dropout = tf.nn.dropout(dense,keep_prob)\n",
    "    classifier_output = keras.layers.Dense(2,activation='softmax',kernel_initializer='glorot_normal')(dropout) \n",
    "    return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gru_cell(keep_prob):\n",
    "    gru_cell = tf.contrib.rnn.GRUCell(512,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    dropout_gru_cell = tf.contrib.rnn.DropoutWrapper(gru_cell,input_keep_prob=keep_prob,output_keep_prob=keep_prob,state_keep_prob=keep_prob)\n",
    "    return dropout_gru_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(sequence_input,initial_state,encoder_output,batch_size,sequence_length,max_sequence_length,vocab_size,embedding_size,keep_prob):\n",
    "    embedding_matrix = tf.get_variable('embedding_matrix',shape=[vocab_size,embedding_size],initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    generator_embed_sequence = tf.nn.embedding_lookup(embedding_matrix,sequence_input)\n",
    "    generator_cell = tf.contrib.rnn.MultiRNNCell([get_gru_cell(keep_prob) for _ in range(1)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    with tf.variable_scope('generator'):\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(generator_embed_sequence,sequence_length=sequence_length,time_major=False)\n",
    "        #attention\n",
    "        training_LuongAttention = tf.contrib.seq2seq.LuongAttention(num_units=512,memory=encoder_output,memory_sequence_length=None)   \n",
    "        training_attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell=generator_cell,attention_mechanism=training_LuongAttention,attention_layer_size=512,alignment_history=False,output_attention=True)    \n",
    "        training_attn_state = training_attn_cell.zero_state(batch_size,tf.float32).clone(cell_state=initial_state) \n",
    "        #attention\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(training_attn_cell,helper=training_helper,initial_state=training_attn_state,output_layer=output_layer)\n",
    "        training_generator_output,training_generator_state,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,output_time_major=False,impute_finished=True,maximum_iterations=max_sequence_length)\n",
    "    with tf.variable_scope('generator',reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([word_to_int['<GO>']],dtype=tf.int32),[batch_size])\n",
    "        #attention\n",
    "        predicting_LuongAttention = tf.contrib.seq2seq.LuongAttention(num_units=512,memory=encoder_output,memory_sequence_length=None)\n",
    "        predicting_attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell=generator_cell,attention_mechanism=predicting_LuongAttention,attention_layer_size=512,alignment_history=True,output_attention=True) \n",
    "        predicting_attn_state = predicting_attn_cell.zero_state(batch_size,tf.float32).clone(cell_state=initial_state) \n",
    "        #attention\n",
    "        predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_matrix,start_tokens,word_to_int['<EOS>'])\n",
    "        predicting_decoder = tf.contrib.seq2seq.BasicDecoder(predicting_attn_cell,predicting_helper,predicting_attn_state,output_layer)\n",
    "        predicting_generator_output,predicting_generator_state,_ = tf.contrib.seq2seq.dynamic_decode(predicting_decoder,output_time_major=False,impute_finished=True,maximum_iterations=max_sequence_length)\n",
    "        attention_matrix = tf.identity(predicting_generator_state.alignment_history.stack(),name='attention_matrix')\n",
    "        #print(attention_matrix)\n",
    "    with tf.variable_scope('generator',reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([word_to_int['<GO>']],dtype=tf.int32),[batch_size])\n",
    "        beamsearch_initial_state = tf.contrib.seq2seq.tile_batch(initial_state,multiplier=3)\n",
    "        #attention\n",
    "        beamsearch_encoder_output = tf.contrib.seq2seq.tile_batch(encoder_output,multiplier=3)\n",
    "        beamsearch_LuongAttention = tf.contrib.seq2seq.LuongAttention(num_units=512,memory=beamsearch_encoder_output,memory_sequence_length=None)\n",
    "        beamsearch_attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell=generator_cell ,attention_mechanism=beamsearch_LuongAttention,attention_layer_size=512,alignment_history=False,output_attention=True) \n",
    "        beamsearch_attn_state = beamsearch_attn_cell.zero_state(batch_size*3,tf.float32).clone(cell_state=beamsearch_initial_state)\n",
    "        #attention\n",
    "        beamsearch_predicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(beamsearch_attn_cell,embedding=embedding_matrix,start_tokens=start_tokens,end_token=word_to_int['<EOS>'],initial_state=beamsearch_attn_state,beam_width=3,output_layer=output_layer,length_penalty_weight=0.6)\n",
    "        beamsearch_generator_output,beamsearch_generator_state,_ = tf.contrib.seq2seq.dynamic_decode(beamsearch_predicting_decoder,output_time_major=False,impute_finished=False,maximum_iterations=max_sequence_length)\n",
    "    return training_generator_output,predicting_generator_output,beamsearch_generator_output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(classifier_output,label,training_generator_output,sequence_output,sequence_length,max_sequence_length,ratio_c,ratio_g,weight,flag):\n",
    "    classifier_loss = tf.reduce_mean(tf.contrib.keras.losses.categorical_crossentropy(label,classifier_output),name='classifier_loss')\n",
    "    classifier_loss_freeze = tf.stop_gradient(classifier_loss)\n",
    "    classifier_loss = tf.where(flag,classifier_loss,classifier_loss_freeze)\n",
    "    training_logits = tf.identity(training_generator_output.rnn_output,name='training_logits')\n",
    "    masks = tf.sequence_mask(sequence_length,max_sequence_length,dtype=tf.float32,name='mask')  \n",
    "    generator_loss = tf.contrib.seq2seq.sequence_loss(training_logits,sequence_output,masks,average_across_timesteps=False,average_across_batch=False)  \n",
    "    generator_loss = tf.reduce_sum(generator_loss,axis=1)\n",
    "    generator_loss = tf.multiply(weight,generator_loss)\n",
    "    generator_loss = tf.reduce_mean(generator_loss,name='generator_loss')\n",
    "    classifier_loss = tf.multiply(ratio_c,classifier_loss)\n",
    "    generator_loss = tf.multiply(ratio_g,generator_loss)\n",
    "    tv = tf.trainable_variables()\n",
    "    reg_loss = tf.reduce_sum([tf.nn.l2_loss(v) for v in tv])\n",
    "    reg_loss_gen = tf.reduce_sum([tf.nn.l2_loss(v) for v in tv if ('generator' in v.name)])\n",
    "    reg_loss = tf.where(flag,reg_loss,reg_loss_gen)\n",
    "    loss = tf.add_n([classifier_loss,generator_loss,0.0001*reg_loss],name='loss')      \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(loss,learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    capped_gradients = [(tf.clip_by_value(grad,-5.,5.),var) for grad,var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(classifier_output):\n",
    "    prediction = tf.identity(classifier_output,name='prediction')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generation(predicting_generator_output,beamsearch_generator_output):\n",
    "    greedysearch_sequence = tf.identity(predicting_generator_output.sample_id,name='greedysearch_sequence')\n",
    "    beamsearch_sequence = tf.identity(beamsearch_generator_output.predicted_ids,name='beamsearch_sequence')\n",
    "    return greedysearch_sequence,beamsearch_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    tf.set_random_seed(1)\n",
    "    with tf.name_scope('inputs'):\n",
    "        img1,img2,img1id,img2id,label,sequence_input,sequence_output,sequence_length,max_sequence_length,batch_size,learning_rate,keep_prob,ratio_c,ratio_g,weight,flag = get_input()\n",
    "    with tf.name_scope('extractor'):\n",
    "        with tf.variable_scope('extractor'):\n",
    "            conv_img1,globalpool_img1 = extractor(img1)\n",
    "        with tf.variable_scope('extractor',reuse=True):\n",
    "            conv_img2,globalpool_img2 = extractor(img2)\n",
    "        with tf.variable_scope('image_to_image_attention'):\n",
    "            features_img1,attn_conv_img1 = image_to_image_attention(conv_img1,globalpool_img2)\n",
    "        with tf.variable_scope('image_to_image_attention',reuse=True):\n",
    "            features_img2,attn_conv_img2 = image_to_image_attention(conv_img2,globalpool_img1)\n",
    "        with tf.variable_scope('img2vec'):\n",
    "            extractor_output_img1 = img2vec(attn_conv_img1)\n",
    "        with tf.variable_scope('img2vec',reuse=True):\n",
    "            extractor_output_img2 = img2vec(attn_conv_img2)\n",
    "        with tf.variable_scope('img_embedding'):\n",
    "            img1_embedding,img2_embedding = img_embedding(img1id,img2id)\n",
    "        extractor_output = tf.concat([extractor_output_img1,extractor_output_img2,img1_embedding,img2_embedding],axis=1)\n",
    "        encoder_output = tf.concat([features_img1,features_img2],axis=1)\n",
    "        encoder_output_freeze = tf.stop_gradient(encoder_output)\n",
    "        extractor_output_freeze = tf.stop_gradient(extractor_output)\n",
    "        encoder_output = tf.where(flag,encoder_output,encoder_output_freeze)\n",
    "        extractor_output = tf.where(flag,extractor_output,extractor_output_freeze)\n",
    "    with tf.name_scope('classifier'):\n",
    "        classifier_output = classifier(extractor_output,keep_prob)\n",
    "    with tf.name_scope('prediction'):\n",
    "        prediction = prediction(classifier_output)  \n",
    "    with tf.name_scope('generator'):\n",
    "        dense_output = keras.layers.Dense(512,activation='tanh',kernel_initializer='glorot_normal')(extractor_output)\n",
    "        initial_state = (dense_output,)    \n",
    "        training_generator_output,predicting_generator_output,beamsearch_generator_output = generator(sequence_input,initial_state,encoder_output,batch_size,sequence_length,max_sequence_length,vocab_size,embedding_size,keep_prob)  \n",
    "    with tf.name_scope('generation'):\n",
    "        greedysearch_sequence,beamsearch_sequence = generation(predicting_generator_output,beamsearch_generator_output) \n",
    "    with tf.name_scope('loss'):\n",
    "        loss = loss(classifier_output,label,training_generator_output,sequence_output,sequence_length,max_sequence_length,ratio_c,ratio_g,weight,flag)\n",
    "    with tf.name_scope('optimizer'): \n",
    "        train_op = optimizer(loss,learning_rate)                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dataset/traindata.dat','r') as f:#in traindata, the first col is img_name of top, the second col is img_name of down(i.e. bottom), the third col is comment_index  \n",
    "    content = f.readlines()\n",
    "traindata = []\n",
    "for line in content:\n",
    "    line = line[:-1].split('\\t')\n",
    "    traindata.append((line[0],line[1],int(line[2])))\n",
    "traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tops_qrel_file_path = 'evaluation/devdata_tops_qrel.dat'\n",
    "tops_trec_file_path = 'evaluation/devdata_tops_trec.dat'\n",
    "#downs_qrel_file_path = 'evaluation/devdata_downs_qrel.dat'\n",
    "#downs_trec_file_path = 'evaluation/devdata_downs_trec.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'dataset/devdata_tops.dat'\n",
    "dev_tops_data,tops_orderlist,model_tops_comments,tops_labellist = prepare_evaluation(data_path,comments,int_to_word,word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_path = 'dataset/devdata_downs.dat'\n",
    "#dev_downs_data,downs_orderlist,model_downs_comments,downs_labellist = prepare_evaluation(data_path,comments,int_to_word,word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tops_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_downs_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "rat_c = 1.0\n",
    "rat_g = 1.0\n",
    "epochs = 5\n",
    "rate = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cla_cost_list = []\n",
    "gen_cost_list = []\n",
    "bleus_tops = []\n",
    "auc_tops = []\n",
    "trec_evals_tops = []\n",
    "#bleus_downs = []\n",
    "#trec_evals_downs = []\n",
    "#auc_downs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beamsearch = True\n",
    "checkpoint = 'checkpoint/trained_model.ckpt'\n",
    "with tf.Session(graph=train_graph,config=config) as sess:\n",
    "    writer = tf.summary.FileWriter('checkpoint/',sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(time.localtime())\n",
    "    classifier_loss = train_graph.get_tensor_by_name('loss/classifier_loss:0')\n",
    "    generator_loss = train_graph.get_tensor_by_name('loss/generator_loss:0')\n",
    "    for epoch in range(epochs):\n",
    "        b_s = 64#batch_size\n",
    "        train_cla_cost = 0\n",
    "        train_gen_cost = 0\n",
    "        temp_cla_cost_list = []\n",
    "        temp_gen_cost_list = []\n",
    "        step = 0\n",
    "        for _,(x_i1,x_i2,x_id1,x_id2,y_l,x_s_i,x_s_o,seq_len,max_seq_len,wei) in enumerate(get_batches(traindata,b_s,comments,toplist,downlist,combinationlist,imglist,topidlist,downidlist,word_to_int['<PAD>'])):\n",
    "            _,cost1,cost2 = sess.run([train_op,classifier_loss,generator_loss],{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,label:y_l,sequence_input:x_s_i,sequence_output:x_s_o,sequence_length:seq_len,max_sequence_length:max_seq_len,batch_size:len(x_i1),learning_rate:lr,keep_prob:rate,ratio_c:rat_c,ratio_g:rat_g,weight:wei,flag:True})   \n",
    "            train_cla_cost += cost1\n",
    "            train_gen_cost += cost2\n",
    "            step += 1\n",
    "            if step%1000 == 0:\n",
    "                temp_cla_cost_list.append(train_cla_cost/step)\n",
    "                temp_gen_cost_list.append(train_gen_cost/step)\n",
    "                print(str(train_cla_cost/step)+'&'+str(train_gen_cost/step)+' '+'pass!')\n",
    "        temp_cla_cost_list.append(train_cla_cost/step)\n",
    "        temp_gen_cost_list.append(train_gen_cost/step)\n",
    "        cla_cost_list.append(temp_cla_cost_list)\n",
    "        gen_cost_list.append(temp_gen_cost_list)\n",
    "        print('Epoch {}/{} - Training Loss: {:.3f}&{:.3f}'.format(epoch+1,epochs,train_cla_cost/step,train_gen_cost/step))\n",
    "        saver.save(sess,checkpoint,global_step=epoch+1)\n",
    "        print('Model Trained and Saved')\n",
    "        print(time.localtime())\n",
    "        #validation       \n",
    "        b_s = 64\n",
    "        max_seq_len = 30\n",
    "        system_tops_comments = {}\n",
    "        tops_trec = {}\n",
    "        query_number = 0\n",
    "        step = 0\n",
    "        for top in tops_orderlist:\n",
    "            downsoftop = dev_tops_data[top]\n",
    "            probabilitylist = {}\n",
    "            for batch_i in range(len(downsoftop)//b_s+1):\n",
    "                start_i = batch_i*b_s\n",
    "                downs = downsoftop[start_i:start_i+b_s]\n",
    "                x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(top,downs,0,imglist,topidlist,downidlist)\n",
    "                seq_len = [30]*len(x_i1)\n",
    "                prob,gred_seq,beam_seq = sess.run([prediction,greedysearch_sequence,beamsearch_sequence],{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,sequence_length:seq_len,max_sequence_length:max_seq_len,batch_size:len(x_i1),keep_prob:1.0,flag:True})\n",
    "                j = 0\n",
    "                for down in downs:\n",
    "                    probabilitylist[down] = prob[j][1]\n",
    "                    if model_tops_comments.get((top,down)) != None:\n",
    "                        if beamsearch:\n",
    "                            system_tops_comments[(top,down)] = [(id_seq_to_word_seq(beam_seq[j][:,index],int_to_word,word_to_int['<EOS>'])).split()[:-1] for index in range(3)]#3 is beam_width\n",
    "                        else:\n",
    "                            system_tops_comments[(top,down)] = (id_seq_to_word_seq(gred_seq[j],int_to_word,word_to_int['<EOS>'])).split()[:-1]\n",
    "                    j += 1 \n",
    "                step += 1\n",
    "                if step%1000 == 0:\n",
    "                    print('pass!')\n",
    "            tops_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "            del probabilitylist,downsoftop\n",
    "            query_number += 1\n",
    "        bleu,_ = bleu_evalaution(model_tops_comments,system_tops_comments,beamsearch)\n",
    "        bleus_tops.append(bleu)\n",
    "        del system_tops_comments\n",
    "        auc_tops.append(auc_evaluation(tops_labellist,tops_trec))\n",
    "        trec_evals_tops.append(trec_evaluation(tops_qrel_file_path,tops_trec_file_path,tops_trec))\n",
    "        del tops_trec\n",
    "        '''\n",
    "        system_downs_comments = {}\n",
    "        downs_trec = {}\n",
    "        query_number = 0\n",
    "        step = 0\n",
    "        for down in downs_orderlist:\n",
    "            topsofdown = dev_downs_data[down]\n",
    "            probabilitylist = {}\n",
    "            for batch_i in range(len(topsofdown)//b_s+1):\n",
    "                start_i = batch_i*b_s\n",
    "                tops = topsofdown[start_i:start_i+b_s]\n",
    "                x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(down,tops,1,imglist,topidlist,downidlist)\n",
    "                seq_len = [30]*len(x_i1)\n",
    "                prob,gred_seq,beam_seq = sess.run([prediction,greedysearch_sequence,beamsearch_sequence],{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,sequence_length:seq_len,max_sequence_length:max_seq_len,batch_size:len(x_i1),keep_prob:1.0,flag:True})\n",
    "                j = 0\n",
    "                for top in tops:\n",
    "                    probabilitylist[top] = prob[j][1]\n",
    "                    if model_downs_comments.get((down,top)) != None:\n",
    "                        if beamsearch:\n",
    "                            system_downs_comments[(down,top)] = [(id_seq_to_word_seq(beam_seq[j][:,index],int_to_word,word_to_int['<EOS>'])).split()[:-1] for index in range(3)]\n",
    "                        else:\n",
    "                            system_downs_comments[(down,top)] = (id_seq_to_word_seq(gred_seq[j],int_to_word,word_to_int['<EOS>'])).split()[:-1]\n",
    "                    j += 1\n",
    "                step += 1\n",
    "                if step%1000 == 0:\n",
    "                    print('pass!')\n",
    "            downs_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "            del probabilitylist,topsofdown\n",
    "            query_number += 1\n",
    "        bleu,_ = bleu_evalaution(model_downs_comments,system_downs_comments,beamsearch)\n",
    "        bleus_downs.append(bleu)\n",
    "        del system_downs_comments\n",
    "        auc_downs.append(auc_evaluation(downs_labellist,downs_trec))\n",
    "        trec_evals_downs.append(trec_evaluation(downs_qrel_file_path,downs_trec_file_path,downs_trec))\n",
    "        del downs_trec\n",
    "        '''\n",
    "        #validation        \n",
    "        print(time.localtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tops_qrel_file_path = 'evaluation/testdata_tops_qrel.dat'\n",
    "tops_trec_file_path = 'evaluation/testdata_tops_trec.dat'\n",
    "downs_qrel_file_path = 'evaluation/testdata_downs_qrel.dat'\n",
    "downs_trec_file_path = 'evaluation/testdata_downs_trec.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'dataset/testdata_tops.dat'#in testdata_tops, the first col is img_name of top, the second col is img_name of down(i.e. bottom), the third col is rel(1 relevant, 0 irrelevant), the fourth col is comments_index(-1 is a special comment_index for irrelevant combination)    \n",
    "test_tops_data,tops_orderlist,model_tops_comments,tops_labellist = prepare_evaluation(data_path,comments,int_to_word,word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'dataset/testdata_downs.dat'#in testdata_downs, the first col is img_name of down(i.e. bottom), the second col is img_name of top, the third col is rel(1 relevant, 0 irrelevant), the fourth col is comments_index(-1 is a special comment_index for irrelevant combination)    \n",
    "test_downs_data,downs_orderlist,model_downs_comments,downs_labellist = prepare_evaluation(data_path,comments,int_to_word,word_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_tops_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_downs_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beamsearch = True\n",
    "print(time.localtime())\n",
    "checkpoint = 'checkpoint/trained_model.ckpt'\n",
    "test_graph = tf.Graph()\n",
    "with tf.Session(graph=test_graph,config=config) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "    loader.restore(sess,checkpoint)\n",
    "    img1 = test_graph.get_tensor_by_name('inputs/img1:0')\n",
    "    img2 = test_graph.get_tensor_by_name('inputs/img2:0')\n",
    "    img1id = test_graph.get_tensor_by_name('inputs/img1id:0')\n",
    "    img2id = test_graph.get_tensor_by_name('inputs/img2id:0')\n",
    "    sequence_length = test_graph.get_tensor_by_name('inputs/sequence_length:0')\n",
    "    max_sequence_length = test_graph.get_tensor_by_name('inputs/max_sequence_length:0')\n",
    "    batch_size = test_graph.get_tensor_by_name('inputs/batch_size:0')\n",
    "    keep_prob = test_graph.get_tensor_by_name('inputs/keep_prob:0')\n",
    "    flag = test_graph.get_tensor_by_name('inputs/flag:0')\n",
    "    prediction = test_graph.get_tensor_by_name('prediction/prediction:0')\n",
    "    greedysearch_sequence = test_graph.get_tensor_by_name('generation/greedysearch_sequence:0')\n",
    "    beamsearch_sequence = test_graph.get_tensor_by_name('generation/beamsearch_sequence:0')\n",
    "    b_s = 64\n",
    "    max_seq_len = 30\n",
    "    system_tops_comments = {}\n",
    "    tops_trec = {}\n",
    "    query_number = 0\n",
    "    step = 0\n",
    "    for top in tops_orderlist:\n",
    "        downsoftop = test_tops_data[top]\n",
    "        probabilitylist = {}\n",
    "        for batch_i in range(len(downsoftop)//b_s+1):\n",
    "            start_i = batch_i*b_s\n",
    "            downs = downsoftop[start_i:start_i+b_s]\n",
    "            x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(top,downs,0,imglist,topidlist,downidlist)\n",
    "            seq_len = [30]*len(x_i1)\n",
    "            prob,gred_seq,beam_seq = sess.run([prediction,greedysearch_sequence,beamsearch_sequence],{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,sequence_length:seq_len,max_sequence_length:max_seq_len,batch_size:len(x_i1),keep_prob:1.0,flag:True})\n",
    "            j = 0\n",
    "            for down in downs:\n",
    "                probabilitylist[down] = prob[j][1]\n",
    "                if model_tops_comments.get((top,down)) != None:\n",
    "                    if beamsearch:\n",
    "                        system_tops_comments[(top,down)] = [(id_seq_to_word_seq(beam_seq[j][:,index],int_to_word,word_to_int['<EOS>'])).split()[:-1] for index in range(3)]\n",
    "                    else:\n",
    "                        system_tops_comments[(top,down)] = (id_seq_to_word_seq(gred_seq[j],int_to_word,word_to_int['<EOS>'])).split()[:-1]\n",
    "                j += 1 \n",
    "            step += 1\n",
    "            if step%1000 == 0:\n",
    "                print('pass!')\n",
    "        tops_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "        del probabilitylist,downsoftop\n",
    "        query_number += 1\n",
    "    _,select_tops = bleu_evalaution(model_tops_comments,system_tops_comments,beamsearch)\n",
    "    auc_evaluation(tops_labellist,tops_trec)\n",
    "    trec_evaluation(tops_qrel_file_path,tops_trec_file_path,tops_trec)\n",
    "    del tops_trec\n",
    "    system_downs_comments = {}\n",
    "    downs_trec = {}\n",
    "    query_number = 0\n",
    "    step = 0\n",
    "    for down in downs_orderlist:\n",
    "        topsofdown = test_downs_data[down]\n",
    "        probabilitylist = {}\n",
    "        for batch_i in range(len(topsofdown)//b_s+1):\n",
    "            start_i = batch_i*b_s\n",
    "            tops = topsofdown[start_i:start_i+b_s]\n",
    "            x_i1,x_i2,x_id1,x_id2 = build_evaluation_batch(down,tops,1,imglist,topidlist,downidlist)\n",
    "            seq_len = [30]*len(x_i1)\n",
    "            prob,gred_seq,beam_seq = sess.run([prediction,greedysearch_sequence,beamsearch_sequence],{img1:x_i1,img2:x_i2,img1id:x_id1,img2id:x_id2,sequence_length:seq_len,max_sequence_length:max_seq_len,batch_size:len(x_i1),keep_prob:1.0,flag:True})\n",
    "            j = 0\n",
    "            for top in tops:\n",
    "                probabilitylist[top] = prob[j][1]\n",
    "                if model_downs_comments.get((down,top)) != None:\n",
    "                    if beamsearch:\n",
    "                        system_downs_comments[(down,top)] = [(id_seq_to_word_seq(beam_seq[j][:,index],int_to_word,word_to_int['<EOS>'])).split()[:-1] for index in range(3)]\n",
    "                    else:\n",
    "                        system_downs_comments[(down,top)] = (id_seq_to_word_seq(gred_seq[j],int_to_word,word_to_int['<EOS>'])).split()[:-1]\n",
    "                j += 1\n",
    "            step += 1\n",
    "            if step%1000 == 0:\n",
    "                print('pass!')\n",
    "        downs_trec[query_number] = sorted(probabilitylist.items(),key=lambda item:item[1],reverse=True)\n",
    "        del probabilitylist,topsofdown\n",
    "        query_number += 1\n",
    "    _,select_downs = bleu_evalaution(model_downs_comments,system_downs_comments,beamsearch)\n",
    "    auc_evaluation(downs_labellist,downs_trec)\n",
    "    trec_evaluation(downs_qrel_file_path,downs_trec_file_path,downs_trec)\n",
    "    del downs_trec\n",
    "print(time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('system_comments/system_tops_comments.dat','w') as f:\n",
    "    if beamsearch:\n",
    "        for combination,commentlist in system_tops_comments.items():\n",
    "            comment = ' '.join(commentlist[select_tops[combination]])\n",
    "            f.write(combination[0]+'\\t'+combination[1]+'\\t'+comment+'\\n')\n",
    "    else:\n",
    "        for combination,comment in system_downs_comments.items():\n",
    "            comment = ' '.join(comment)\n",
    "            f.write(combination[0]+'\\t'+combination[1]+'\\t'+comment+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('system_comments/system_downs_comments.dat','w') as f:\n",
    "    if beamsearch:\n",
    "        for combination,commentlist in system_downs_comments.items():\n",
    "            comment = ' '.join(commentlist[select_downs[combination]])\n",
    "            f.write(combination[0]+'\\t'+combination[1]+'\\t'+comment+'\\n')\n",
    "    else:\n",
    "        for combination,comment in system_downs_comments.items():\n",
    "            comment = ' '.join(comment)\n",
    "            f.write(combination[0]+'\\t'+combination[1]+'\\t'+comment+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
